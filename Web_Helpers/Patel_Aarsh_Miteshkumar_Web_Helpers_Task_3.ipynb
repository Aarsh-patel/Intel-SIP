{"cells":[{"cell_type":"markdown","metadata":{"id":"3EgrDw47fzxr"},"source":["# Different approaches of data augmentation are listed below :-\n","\n","\n","1. **Random Insertion:** Inserting this identified synonym at some random position in the sentence and this word is not in stopwords.\n","\n","2. **Random Deletion:** Randomly removing words within the sentence.\n","\n","3. **Random Swapping:** Randomly choose two words within the sentence and swap their positions.\n","4. **Backtranslation:** A sentence is translated in one language and then a new sentence is translated again in the original language. So, different sentences are created.\n","5. **Generative Models:** A generative adversarial network (GAN) is trained to generate text with a few words and generative language models like BERT, RoBERTa, BART and T5 model can be used to generate the text in a more class category preserving manner."]},{"cell_type":"markdown","metadata":{"id":"mtiLUFgak_F9"},"source":["# We will be utilizing nlpaug library for data set augmentation using random insertion method"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13332,"status":"ok","timestamp":1696264037419,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"Dz9p6EF0jfHs","outputId":"90aa78d0-bf67-486c-8a29-a626bddf2e81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n","Collecting keras_preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n","Installing collected packages: keras_preprocessing\n","Successfully installed keras_preprocessing-1.1.2\n"]}],"source":["!pip install nlpaug\n","!pip install keras_preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":628,"status":"ok","timestamp":1696264062091,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"cP2R3XWUjTez","outputId":"cb46ff26-60fd-443e-a440-c883e0ac161b"},"outputs":[{"output_type":"stream","name":"stdout","text":["I be rich\n"]}],"source":["import nlpaug.augmenter.word as naw\n","aug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng',\n","                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False,\n","                     verbose=0)\n","test_sentence = \"I am rich\"\n","test_sentence_aug = aug.augment(test_sentence)\n","print(test_sentence_aug[0])"]},{"cell_type":"markdown","metadata":{"id":"3UKcAXTplobd"},"source":["# Augmenting the data to the original dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8oSl6i1pjkf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import bz2\n","import os\n","import re\n","import gc\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.utils import pad_sequences\n","\n","from tensorflow.keras import models, layers, optimizers\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMLp9yj1lwNn"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TI15MedlmGDZ"},"outputs":[],"source":["def assign_labels_and_comments(file):\n","    labels = []\n","    extra_labels = []\n","    comments = []\n","    extra_comments = []\n","    i=0\n","    for line in bz2.BZ2File(file):\n","        i+=1\n","        if i>20000:\n","          break\n","        x = line.decode(\"utf-8\")\n","        labels.append(int(x[9]) - 1)\n","        extra_labels.append(int(x[9]) - 1)\n","        comments.append(x[10:].strip())\n","        x = aug.augment(x[10:])\n","        extra_comments.append(x[0].strip())\n","    return labels, comments, extra_labels, extra_comments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5uoOUHhmJH8"},"outputs":[],"source":["train_labels, train_comments, et_l, et_c = assign_labels_and_comments('/content/drive/MyDrive/Intel SIP/train.ft.txt.bz2')\n","test_labels, test_comments, ete_l, ete_c = assign_labels_and_comments('/content/drive/MyDrive/Intel SIP/test.ft.txt.bz2')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1696252880007,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"hAOFayj6pteu","outputId":"a24aa208-4e64-4710-d3fc-c0ce605f93ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of dataset before augmentation 40000\n","Size of dataset after augmentation 80000\n"]}],"source":["print(f\"Size of dataset before augmentation {len(train_labels)+len(test_labels)}\")\n","train_labels.extend(et_l)\n","test_labels.extend(ete_l)\n","train_labels = np.array(train_labels)\n","test_labels = np.array(test_labels)\n","train_comments.extend(et_c)\n","test_comments.extend(ete_c)\n","print(f\"Size of dataset after augmentation {len(train_labels)+len(test_labels)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ih_dQUK2pCJh"},"outputs":[],"source":["not_alphanumeric = re.compile(r'[\\W]')\n","not_ascii = re.compile(r'[^a-z0-1\\s]')\n","def processed_comments(texts):\n","    processed_comments = []\n","    for text in texts:\n","        lower = text.lower()\n","        no_punctuation = not_alphanumeric.sub(r' ', lower)\n","        no_non_ascii = not_ascii.sub(r'', no_punctuation)\n","        processed_comments.append(no_non_ascii)\n","    return processed_comments\n","train_comments = processed_comments(train_comments)\n","test_comments = processed_comments(test_comments)\n","train_comments, val_comments, train_labels, val_labels = train_test_split(train_comments, train_labels, random_state=42, test_size=0.2)\n","maximum_features = 14000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=maximum_features)\n","tokenizer.fit_on_texts(train_comments)\n","train_comments = tokenizer.texts_to_sequences(train_comments)\n","val_comments = tokenizer.texts_to_sequences(val_comments)\n","test_comments = tokenizer.texts_to_sequences(test_comments)\n","maximum_length = max(len(train_ex) for train_ex in train_comments)\n","train_comments_pad = tf.keras.preprocessing.sequence.pad_sequences(train_comments, maxlen=maximum_length)\n","val_comments_pad = tf.keras.preprocessing.sequence.pad_sequences(val_comments, maxlen=maximum_length)\n","test_comments_pad = tf.keras.preprocessing.sequence.pad_sequences(test_comments, maxlen=maximum_length)\n","del train_comments, val_comments, test_comments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8317,"status":"ok","timestamp":1696252896655,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"PHAMvVw5sn9A","outputId":"ca761efe-4be4-42c7-e008-730dd7fcc773"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 205)]             0         \n","                                                                 \n"," embedding (Embedding)       (None, 205, 64)           896000    \n","                                                                 \n"," conv1d (Conv1D)             (None, 203, 64)           12352     \n","                                                                 \n"," batch_normalization (Batch  (None, 203, 64)           256       \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 67, 64)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 63, 64)            20544     \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 63, 64)            256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 12, 64)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 8, 64)             20544     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 64)                0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," flatten (Flatten)           (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 100)               6500      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 956553 (3.65 MB)\n","Trainable params: 956297 (3.65 MB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n"]}],"source":["def cnn_model():\n","    sequences = layers.Input(shape=(maximum_length,))\n","    embedded = layers.Embedding(maximum_features, 64)(sequences)\n","    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = layers.MaxPool1D(3)(x)\n","    x = layers.Conv1D(64, 5, activation='relu')(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = layers.MaxPool1D(5)(x)\n","    x = layers.Conv1D(64, 5, activation='relu')(x)\n","    x = layers.GlobalMaxPool1D()(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(100, activation='relu')(x)\n","    predictions = layers.Dense(1, activation='sigmoid')(x)\n","    model = models.Model(inputs=sequences, outputs=predictions)\n","    model.compile(\n","        optimizer='rmsprop',\n","        loss='binary_crossentropy',\n","        metrics=['binary_accuracy']\n","    )\n","    model.summary()\n","    return model\n","\n","model = cnn_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41192,"status":"ok","timestamp":1696252937834,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"0-FmLP5ost-a","outputId":"7362097e-3f7c-4635-924b-9bc7cae167bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","63/63 [==============================] - 23s 180ms/step - loss: 0.5861 - binary_accuracy: 0.7106 - val_loss: 0.6917 - val_binary_accuracy: 0.5077\n","Epoch 2/3\n","63/63 [==============================] - 8s 127ms/step - loss: 0.2222 - binary_accuracy: 0.9137 - val_loss: 0.6960 - val_binary_accuracy: 0.5077\n","Epoch 3/3\n","63/63 [==============================] - 10s 164ms/step - loss: 0.0945 - binary_accuracy: 0.9675 - val_loss: 0.7083 - val_binary_accuracy: 0.5077\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7be422e683a0>"]},"metadata":{},"execution_count":16}],"source":["model.fit(\n","    train_comments_pad,\n","    train_labels,\n","    batch_size=512,\n","    epochs=3,\n","    validation_data=(val_comments_pad, val_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1696252937835,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"bOQAQknJvl8c","outputId":"083f38d6-fdd5-4fd1-af7e-e31132d7410b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 205)]             0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 205, 64)           896000    \n","                                                                 \n"," cu_dnngru (CuDNNGRU)        (None, 205, 128)          74496     \n","                                                                 \n"," cu_dnngru_1 (CuDNNGRU)      (None, 128)               99072     \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                4128      \n","                                                                 \n"," dense_3 (Dense)             (None, 100)               3300      \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 1077097 (4.11 MB)\n","Trainable params: 1077097 (4.11 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["def rnn_model():\n","    sequences = layers.Input(shape=(maximum_length,))\n","    embedded = layers.Embedding(maximum_features, 64)(sequences)\n","    x = tf.compat.v1.keras.layers.CuDNNGRU(128, return_sequences=True)(embedded)\n","    x = tf.compat.v1.keras.layers.CuDNNGRU(128)(x)\n","    x = layers.Dense(32, activation='relu')(x)\n","    x = layers.Dense(100, activation='relu')(x)\n","    predictions = layers.Dense(1, activation='sigmoid')(x)\n","    model = models.Model(inputs=sequences, outputs=predictions)\n","    model.compile(\n","        optimizer='rmsprop',\n","        loss='binary_crossentropy',\n","        metrics=['binary_accuracy']\n","    )\n","    model.summary()\n","    return model\n","\n","rnn_model = rnn_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35929,"status":"ok","timestamp":1696252973748,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"},"user_tz":-330},"id":"TD0mN0ITvn_s","outputId":"908b881a-43a2-4095-854b-9bc0d274a54a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","63/63 [==============================] - 17s 206ms/step - loss: 0.6917 - binary_accuracy: 0.5168 - val_loss: 0.6854 - val_binary_accuracy: 0.5695\n","Epoch 2/3\n","63/63 [==============================] - 11s 168ms/step - loss: 0.6083 - binary_accuracy: 0.6812 - val_loss: 0.4765 - val_binary_accuracy: 0.7950\n","Epoch 3/3\n","63/63 [==============================] - 8s 126ms/step - loss: 0.4229 - binary_accuracy: 0.8109 - val_loss: 0.3531 - val_binary_accuracy: 0.8494\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7be4210b2170>"]},"metadata":{},"execution_count":18}],"source":["rnn_model.fit(train_comments_pad,\n","    train_labels,\n","    batch_size=512,\n","    epochs=3,\n","    validation_data=(val_comments_pad, val_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0pMTxr5vqo1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696252982054,"user_tz":-330,"elapsed":8316,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"outputId":"c5d49436-f31f-44de-9842-dbaa983eff38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 58537 unique tokens.\n"]}],"source":["MAX_NB_WORDS = 10000\n","MAX_SEQUENCE_LENGTH = 250\n","EMBEDDING_DIM = 100\n","def get_labels_and_texts(file):\n","    labels = []\n","    texts = []\n","    i = 0\n","    for line in bz2.BZ2File(file):\n","        i+=1\n","        if i>20000:\n","          break\n","        x = line.decode(\"utf-8\")\n","        labels.append(int(x[9]) - 1)\n","        texts.append(x[10:].strip())\n","    return np.array(labels), texts\n","train_labels, train_texts = get_labels_and_texts('/content/drive/MyDrive/Intel SIP/train.ft.txt.bz2')\n","test_labels, test_texts = get_labels_and_texts('/content/drive/MyDrive/Intel SIP/test.ft.txt.bz2')\n","train_df=pd.DataFrame(zip(train_texts,train_labels),columns=['text','label'])\n","test_df=pd.DataFrame(zip(test_texts,test_labels),columns=['text','label'])\n","import regex as re\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from nltk.tokenize import TreebankWordTokenizer\n","from nltk.stem.regexp import RegexpStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","def remove_special_characters(text):\n","  text=text.str.lower()\n","  text=text.apply(lambda x: re.sub(r'[0-9]+','',x))\n","  text=text.apply(lambda x: re.sub(r'@mention',' ',x))\n","  text=text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', ' ',x))\n","  text=text.apply(lambda x: re.sub(r\"www.\\[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ',x))\n","  text=text.apply(lambda x: re.sub(r\"[_\\,\\>\\(\\-:\\)\\\\\\/\\!\\.\\^\\!\\:\\];='#]\",'',x))\n","  return text\n","train_df['text']=remove_special_characters(train_df['text'])\n","test_df['text']=remove_special_characters(test_df['text'])\n","from keras.preprocessing import text,sequence\n","\n","\n","tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df['text'].values)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","from keras_preprocessing.sequence import pad_sequences\n","\n","train_text = tokenizer.texts_to_sequences(train_df['text'].values)\n","train_text = pad_sequences(train_text, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","y = pd.get_dummies(train_df['label']).values\n","X_train, X_test, Y_train, Y_test = train_test_split(train_text,y, test_size = 0.10, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7hSwTX6v5rk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696252982887,"user_tz":-330,"elapsed":843,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"outputId":"6672a4f1-12cb-4838-ec2f-b2b75a11828c"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 250, 100)          1000000   \n","                                                                 \n"," spatial_dropout1d (Spatial  (None, 250, 100)          0         \n"," Dropout1D)                                                      \n","                                                                 \n"," lstm (LSTM)                 (None, 100)               80400     \n","                                                                 \n"," dense_5 (Dense)             (None, 128)               12928     \n","                                                                 \n"," dense_6 (Dense)             (None, 2)                 258       \n","                                                                 \n","=================================================================\n","Total params: 1093586 (4.17 MB)\n","Trainable params: 1093586 (4.17 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense,Embedding,LSTM,Dropout,SpatialDropout1D,GlobalMaxPooling1D, Dense\n","import tensorflow as tf\n","\n","model = Sequential()\n","model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=train_text.shape[1]))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dense(2, activation='softmax'))\n","\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","source":["epochs = 3\n","batch_size = 512\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vG3F64DymFxw","executionInfo":{"status":"ok","timestamp":1696253779527,"user_tz":-330,"elapsed":142559,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"outputId":"9891bc54-e6cf-4eee-804e-0fae0543a532"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 46s 1s/step - loss: 0.1111 - accuracy: 0.9607 - val_loss: 0.3431 - val_accuracy: 0.8928\n","Epoch 2/3\n","32/32 [==============================] - 46s 1s/step - loss: 0.0871 - accuracy: 0.9719 - val_loss: 0.3745 - val_accuracy: 0.8900\n","Epoch 3/3\n","32/32 [==============================] - 43s 1s/step - loss: 0.0669 - accuracy: 0.9787 - val_loss: 0.4328 - val_accuracy: 0.8911\n"]}]},{"cell_type":"markdown","source":["# Conclusion\n","\n","* **LSTM** based model would be the preferred model due to its high accuracy and it's low loss score.  \n","* Data augmentation helped the model's be more accurate and the training could be performed on a much bigger dataset than previous with very low effort.\n","* Data augmentation was worth the time.\n","* We should continuosly improve the dataset by adding further human made comments and adding the augmented data to improve the model further.\n","\n","\n"],"metadata":{"id":"ML6b_J4pMnBy"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1kTn87Ve53LgD_sZ726oE5wWAQKy3fQhC","authorship_tag":"ABX9TyNClJQGwRrj9wZmv+3GSvlF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}