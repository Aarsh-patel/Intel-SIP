{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMnVDxNC+VKdPfhkRSwZrBM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install keras_preprocessing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROLDdfG7Gua3","executionInfo":{"status":"ok","timestamp":1696090465241,"user_tz":-330,"elapsed":46500,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"outputId":"6646fc43-88e4-4f07-eb6e-bda10a0abd61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting keras_preprocessing\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n","Installing collected packages: keras_preprocessing\n","Successfully installed keras_preprocessing-1.1.2\n"]}]},{"cell_type":"markdown","source":["#\\#Top three models for sentiment analysis are :\n","\n","\n","1.   CNN Models\n","2.   RNN Models\n","3.   LSTM-based Models\n"],"metadata":{"id":"C6v0QCnnikKm"}},{"cell_type":"markdown","source":["#Imports and pre-processing"],"metadata":{"id":"g9MiCg_RFUJk"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import bz2\n","import os\n","import re\n","import gc\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.utils import pad_sequences\n","\n","from tensorflow.keras import models, layers, optimizers\n","\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"9FG4d3VWFOo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def assign_labels_and_comments(file):\n","    labels = []\n","    comments = []\n","    i=0\n","    for line in bz2.BZ2File(file):\n","        i+=1\n","        if i>20000:\n","          break\n","        x = line.decode(\"utf-8\")\n","        labels.append(int(x[9]) - 1)\n","        comments.append(x[10:].strip())\n","    return np.array(labels), comments"],"metadata":{"id":"fd3sv8R9FS31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels, train_comments = assign_labels_and_comments('/content/drive/MyDrive/Intel SIP/train.ft.txt.bz2')\n","test_labels, test_comments = assign_labels_and_comments('/content/drive/MyDrive/Intel SIP/test.ft.txt.bz2')\n","not_alphanumeric = re.compile(r'[\\W]')\n","not_ascii = re.compile(r'[^a-z0-1\\s]')\n","def processed_comments(texts):\n","    processed_comments = []\n","    for text in texts:\n","        lower = text.lower()\n","        no_punctuation = not_alphanumeric.sub(r' ', lower)\n","        no_non_ascii = not_ascii.sub(r'', no_punctuation)\n","        processed_comments.append(no_non_ascii)\n","    return processed_comments\n","train_comments = processed_comments(train_comments)\n","test_comments = processed_comments(test_comments)\n","train_comments, val_comments, train_labels, val_labels = train_test_split(train_comments, train_labels, random_state=42, test_size=0.2)\n","maximum_features = 14000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=maximum_features)\n","tokenizer.fit_on_texts(train_comments)\n","train_comments = tokenizer.texts_to_sequences(train_comments)\n","val_comments = tokenizer.texts_to_sequences(val_comments)\n","test_comments = tokenizer.texts_to_sequences(test_comments)\n","maximum_length = max(len(train_ex) for train_ex in train_comments)\n","train_comments_pad = tf.keras.preprocessing.sequence.pad_sequences(train_comments, maxlen=maximum_length)\n","val_comments_pad = tf.keras.preprocessing.sequence.pad_sequences(val_comments, maxlen=maximum_length)\n","test_comments_pad = tf.keras.preprocessing.sequence.pad_sequences(test_comments, maxlen=maximum_length)\n","del train_comments, val_comments, test_comments"],"metadata":{"id":"pVm5Kl69FdIN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. CNN Models\n","\n","\n","\n","The CNN (Convolutional Neural Network) model for sentiment analysis is a deep learning approach that uses a neural network to classify the sentiment of text data. This model is particularly effective for analyzing large datasets, as it can learn to recognize patterns and relationships within the data.\n","\n","In this model, the text data is first preprocessed and converted into numerical vectors. These vectors are then fed into a convolutional layer, which applies filters to the input data to extract features that are relevant to the sentiment analysis task. The output of the convolutional layer is then passed through a pooling layer, which reduces the dimensionality of the data and helps to prevent overfitting.\n","\n","\n","\n","## Evaluation metric\n","F1 score: The F1 score combines precision and recall into a single metric, providing a balanced measure of model performance. It is especially useful when there is an imbalance between the classes.\n","\n","## Computation time\n","Computation is usually the least of the 3 methods.\n"],"metadata":{"id":"CbRv6gk6jZZ-"}},{"cell_type":"code","source":["def cnn_model():\n","    sequences = layers.Input(shape=(maximum_length,))\n","    embedded = layers.Embedding(maximum_features, 64)(sequences)\n","    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = layers.MaxPool1D(3)(x)\n","    x = layers.Conv1D(64, 5, activation='relu')(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = layers.MaxPool1D(5)(x)\n","    x = layers.Conv1D(64, 5, activation='relu')(x)\n","    x = layers.GlobalMaxPool1D()(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(100, activation='relu')(x)\n","    predictions = layers.Dense(1, activation='sigmoid')(x)\n","    model = models.Model(inputs=sequences, outputs=predictions)\n","    model.compile(\n","        optimizer='rmsprop',\n","        loss='binary_crossentropy',\n","        metrics=['binary_accuracy']\n","    )\n","    model.summary()\n","    return model\n","\n","model = cnn_model()"],"metadata":{"id":"Lb9c7z3GF0SE","executionInfo":{"status":"ok","timestamp":1696091362247,"user_tz":-330,"elapsed":3506,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"199f55e1-7f06-45c0-a751-e817ebdfbeac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 203)]             0         \n","                                                                 \n"," embedding (Embedding)       (None, 203, 64)           896000    \n","                                                                 \n"," conv1d (Conv1D)             (None, 201, 64)           12352     \n","                                                                 \n"," batch_normalization (Batch  (None, 201, 64)           256       \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 67, 64)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 63, 64)            20544     \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 63, 64)            256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 12, 64)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 8, 64)             20544     \n","                                                                 \n"," global_max_pooling1d (Glob  (None, 64)                0         \n"," alMaxPooling1D)                                                 \n","                                                                 \n"," flatten (Flatten)           (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 100)               6500      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 956553 (3.65 MB)\n","Trainable params: 956297 (3.65 MB)\n","Non-trainable params: 256 (1.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.fit(\n","    train_comments_pad,\n","    train_labels,\n","    batch_size=512,\n","    epochs=3,\n","    validation_data=(val_comments_pad, val_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_J7Ew-M_F1BD","executionInfo":{"status":"ok","timestamp":1696091407289,"user_tz":-330,"elapsed":42608,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"outputId":"cf24ac4a-deb7-47a1-e8bf-203b74d36eb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 19s 198ms/step - loss: 0.7227 - binary_accuracy: 0.5850 - val_loss: 0.6926 - val_binary_accuracy: 0.4827\n","Epoch 2/3\n","32/32 [==============================] - 6s 174ms/step - loss: 0.3675 - binary_accuracy: 0.8354 - val_loss: 0.6912 - val_binary_accuracy: 0.4680\n","Epoch 3/3\n","32/32 [==============================] - 4s 120ms/step - loss: 0.1902 - binary_accuracy: 0.9263 - val_loss: 0.6931 - val_binary_accuracy: 0.4658\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7da651198f40>"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# 2. RNN Model\n","A Recurrent Neural Network (RNN) is a type of deep learning model that is well-suited for sequence-based tasks, such as sentiment analysis. In an RNN, the output from the previous step is fed back into the model as input for the current step, allowing the model to learn from previous inputs and make predictions based on context.\n","\n","For sentiment analysis, an RNN can be trained on a dataset of text samples labeled with positive, negative, or neutral sentiment. The model learns to identify patterns in the text that are indicative of each sentiment class, and can then be used to predict the sentiment of new text samples.\n","\n","\n","\n","## Evaluation metric\n","Cross-validation: Cross-validation is a technique used to assess the model's performance on multiple subsets of the data. It helps in estimating how well the model will generalize to unseen data.\n","\n","## Computation time\n","Computation is usually in-line with the CNN model.\n"],"metadata":{"id":"ZNAVbAwjlTGD"}},{"cell_type":"code","source":["def rnn_model():\n","    sequences = layers.Input(shape=(maximum_length,))\n","    embedded = layers.Embedding(maximum_features, 64)(sequences)\n","    x = tf.compat.v1.keras.layers.CuDNNGRU(128, return_sequences=True)(embedded)\n","    x = tf.compat.v1.keras.layers.CuDNNGRU(128)(x)\n","    x = layers.Dense(32, activation='relu')(x)\n","    x = layers.Dense(100, activation='relu')(x)\n","    predictions = layers.Dense(1, activation='sigmoid')(x)\n","    model = models.Model(inputs=sequences, outputs=predictions)\n","    model.compile(\n","        optimizer='rmsprop',\n","        loss='binary_crossentropy',\n","        metrics=['binary_accuracy']\n","    )\n","    model.summary()\n","    return model\n","\n","rnn_model = rnn_model()"],"metadata":{"id":"8I3alp9TF7BM","executionInfo":{"status":"ok","timestamp":1696091407290,"user_tz":-330,"elapsed":20,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1c88f66-bbbc-419a-a49f-7b6edde2ef3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 203)]             0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 203, 64)           896000    \n","                                                                 \n"," cu_dnngru (CuDNNGRU)        (None, 203, 128)          74496     \n","                                                                 \n"," cu_dnngru_1 (CuDNNGRU)      (None, 128)               99072     \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                4128      \n","                                                                 \n"," dense_3 (Dense)             (None, 100)               3300      \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 101       \n","                                                                 \n","=================================================================\n","Total params: 1077097 (4.11 MB)\n","Trainable params: 1077097 (4.11 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["rnn_model.fit(train_comments_pad,\n","    train_labels,\n","    batch_size=512,\n","    epochs=3,\n","    validation_data=(val_comments_pad, val_labels))"],"metadata":{"id":"Pfd6B9s4F8aL","executionInfo":{"status":"ok","timestamp":1696091459304,"user_tz":-330,"elapsed":43428,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8bf8480e-c2d7-4464-a1a6-d71e3dc99463"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 13s 255ms/step - loss: 0.6931 - binary_accuracy: 0.5064 - val_loss: 0.6923 - val_binary_accuracy: 0.5343\n","Epoch 2/3\n","32/32 [==============================] - 5s 155ms/step - loss: 0.6927 - binary_accuracy: 0.5128 - val_loss: 0.6938 - val_binary_accuracy: 0.4658\n","Epoch 3/3\n","32/32 [==============================] - 6s 198ms/step - loss: 0.6903 - binary_accuracy: 0.5391 - val_loss: 0.6902 - val_binary_accuracy: 0.4925\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7da6519ff460>"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["#Lstm-preprocessing"],"metadata":{"id":"13Nzj7IFGXay"}},{"cell_type":"code","source":["MAX_NB_WORDS = 10000\n","MAX_SEQUENCE_LENGTH = 250\n","EMBEDDING_DIM = 100"],"metadata":{"id":"h7AYvExxIYAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_labels_and_texts(file):\n","    labels = []\n","    texts = []\n","    i = 0\n","    for line in bz2.BZ2File(file):\n","        i+=1\n","        if i>20000:\n","          break\n","        x = line.decode(\"utf-8\")\n","        labels.append(int(x[9]) - 1)\n","        texts.append(x[10:].strip())\n","    labels = labels[:int(len(labels)*0.01)]\n","    texts = texts[:int(len(texts)*0.01)]\n","    return np.array(labels), texts\n","train_labels, train_texts = get_labels_and_texts('/content/drive/MyDrive/Intel SIP/train.ft.txt.bz2')\n","test_labels, test_texts = get_labels_and_texts('/content/drive/MyDrive/Intel SIP/test.ft.txt.bz2')\n","train_df=pd.DataFrame(zip(train_texts,train_labels),columns=['text','label'])\n","test_df=pd.DataFrame(zip(test_texts,test_labels),columns=['text','label'])\n","import regex as re\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","from nltk.tokenize import TreebankWordTokenizer\n","from nltk.stem.regexp import RegexpStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","def remove_special_characters(text):\n","  text=text.str.lower()\n","  text=text.apply(lambda x: re.sub(r'[0-9]+','',x))\n","  text=text.apply(lambda x: re.sub(r'@mention',' ',x))\n","  text=text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', ' ',x))\n","  text=text.apply(lambda x: re.sub(r\"www.\\[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ',x))\n","  text=text.apply(lambda x: re.sub(r\"[_\\,\\>\\(\\-:\\)\\\\\\/\\!\\.\\^\\!\\:\\];='#]\",'',x))\n","  return text\n","train_df['text']=remove_special_characters(train_df['text'])\n","test_df['text']=remove_special_characters(test_df['text'])\n","from keras.preprocessing import text,sequence\n","\n","\n","tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_df['text'].values)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","from keras_preprocessing.sequence import pad_sequences\n","\n","train_text = tokenizer.texts_to_sequences(train_df['text'].values)\n","train_text = pad_sequences(train_text, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","y = pd.get_dummies(train_df['label']).values\n","X_train, X_test, Y_train, Y_test = train_test_split(train_text,y, test_size = 0.10, random_state = 42)"],"metadata":{"id":"XfMO5ygTGDHN","executionInfo":{"status":"ok","timestamp":1696091470163,"user_tz":-330,"elapsed":8611,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cecdc609-cb61-4a13-8ff8-bb8587f0f284"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3447 unique tokens.\n"]}]},{"cell_type":"markdown","source":["# 3. LSTM-based Model\n","The main advantage of LSTM-based semantic analysis is its ability to capture long-term dependencies in the text. Unlike traditional Bag of Words models, LSTMs can take into account the sequence of words and capture the context and meaning of the text. This allows for more accurate analysis of the text and better performance in tasks that require understanding the meaning of the text.\n","\n","\n","\n","## Evaluation metric\n","Confusion matrix: A confusion matrix provides a detailed breakdown of the model's predictions, showing the number of true positives, true negatives, false positives, and false negatives. It helps in understanding the specific types of errors made by the model.\n","\n","## Computation time\n","Computation is usually the most of the 3 methods.\n"],"metadata":{"id":"tUAUoRGSkqlD"}},{"cell_type":"code","source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense,Embedding,LSTM,Dropout,SpatialDropout1D,GlobalMaxPooling1D, Dense\n","import tensorflow as tf\n","\n","model = Sequential()\n","model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=train_text.shape[1]))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dense(2, activation='softmax'))\n","\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"metadata":{"id":"1QSKZpi0GS_D","executionInfo":{"status":"ok","timestamp":1696091474768,"user_tz":-330,"elapsed":1958,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f91091da-1385-44b4-f228-03c6ac86b54c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 250, 100)          1000000   \n","                                                                 \n"," spatial_dropout1d (Spatial  (None, 250, 100)          0         \n"," Dropout1D)                                                      \n","                                                                 \n"," lstm (LSTM)                 (None, 100)               80400     \n","                                                                 \n"," dense_5 (Dense)             (None, 128)               12928     \n","                                                                 \n"," dense_6 (Dense)             (None, 2)                 258       \n","                                                                 \n","=================================================================\n","Total params: 1093586 (4.17 MB)\n","Trainable params: 1093586 (4.17 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["epochs = 3\n","batch_size = 128\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)"],"metadata":{"id":"imo_jRXiGVIE","executionInfo":{"status":"ok","timestamp":1696091523908,"user_tz":-330,"elapsed":8198,"user":{"displayName":"Patel Aarsh Miteshkumar 21BCE5777","userId":"01626825022078948870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1a4e2e9-5c42-4aef-def9-3be87fe46354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","2/2 [==============================] - 2s 1s/step - loss: 0.5487 - accuracy: 0.7654 - val_loss: 0.8359 - val_accuracy: 0.3889\n","Epoch 2/3\n","2/2 [==============================] - 2s 1s/step - loss: 0.5319 - accuracy: 0.8580 - val_loss: 0.8513 - val_accuracy: 0.4444\n","Epoch 3/3\n","2/2 [==============================] - 2s 1s/step - loss: 0.4133 - accuracy: 0.9444 - val_loss: 0.6977 - val_accuracy: 0.5556\n"]}]},{"cell_type":"markdown","source":["# Conclusion:\n","\n","\n","### LSTM (Long Short-Term Memory) based model would be preferred in production for sentiment analysis compared to CNN (Convolutional Neural Network) and traditional RNN models. The reasons being mentioned below:\n","\n","1. ***Capturing Long-Term Dependencies***: LSTM models are designed to capture long-term dependencies in sequential data. Sentences can have complex structures and dependencies between words that extend over long distances. LSTMs with their memory cells and gates can effectively capture and remember these dependencies, making them better suited for sentiment analysis tasks.\n","\n","2. ***Handling Variable-Length Sequences***: Sentences in sentiment analysis can vary in length. LSTM models can handle variable-length sequences by processing the input step-by-step, dynamically adjusting their internal state based on the input at each time step. This flexibility makes LSTMs more suitable for sentiment analysis tasks where the length of input text can vary.\n","\n","3. ***Dealing with Contextual Information***: Sentiment analysis often requires understanding the context and meaning of words within a sentence. LSTMs excel at capturing contextual information as they maintain an internal memory state that can retain relevant information from earlier parts of the sentence. This allows LSTMs to better understand the sentiment expressed in the text.\n"],"metadata":{"id":"UFtjbbBgxffP"}}]}